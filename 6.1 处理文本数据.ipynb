{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本向量化的多种实现方法：   \n",
    "* 将文本分割为单词，并将每个单词转化为一个向量\n",
    "* 将文本分割为字符，并将每个字符转换为一个向量   \n",
    "* 提取单词或字符的n-gram，并将每个n-gram转换为一个向量。应用于轻量级的浅层文本处理模型（logistics回归和随机森林）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单词和字符的One-hot编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词级别的one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建每个单词的索引\n",
    "token_index = {}\n",
    "\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1             #索引编号0没有指定单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 1,\n",
       " 'ate': 8,\n",
       " 'cat': 2,\n",
       " 'dog': 7,\n",
       " 'homework': 10,\n",
       " 'mat.': 6,\n",
       " 'my': 9,\n",
       " 'on': 4,\n",
       " 'sat': 3,\n",
       " 'the': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index.get('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对样本进行分词，只考虑每个样本钱max_length个单词\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples), max_length, len(token_index)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用keras 实现单词级的one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建单词索引\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras_preprocessing.text.Tokenizer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为整数索引组成的列表\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以直接得到one-hot 二进制表示\n",
    "one_hot_results =tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "type(one_hot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 7,\n",
       " 'cat': 2,\n",
       " 'dog': 6,\n",
       " 'homework': 9,\n",
       " 'mat': 5,\n",
       " 'my': 8,\n",
       " 'on': 4,\n",
       " 'sat': 3,\n",
       " 'the': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用词嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/编码方式.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取词嵌入的两种方法：    \n",
    "* 在完成任务的同时学习词嵌入。一开始是随机的词向量，然后对词向量进行学习，学习方式与学习神经网络的权重相同。   \n",
    "* 在不同于待解决问题的机器学习任务上预计算词嵌入，然后加载到模型中，这些词嵌入又叫做**预训练词嵌入**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用Embedding层学习嵌入空间   \n",
    "![Embedding](./images/Embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000  # 作为特征的单词个数\n",
    "maxien = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将整数列表转换为形状为（samples, maxlen）的二维整数张量\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxien)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxien))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 287us/step - loss: 0.6661 - acc: 0.6279 - val_loss: 0.6128 - val_acc: 0.6996\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 0.5378 - acc: 0.7509 - val_loss: 0.5231 - val_acc: 0.7314\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.4607 - acc: 0.7863 - val_loss: 0.4980 - val_acc: 0.7454\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 77us/step - loss: 0.4220 - acc: 0.8091 - val_loss: 0.4925 - val_acc: 0.7536\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.3944 - acc: 0.8261 - val_loss: 0.4917 - val_acc: 0.7558\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 83us/step - loss: 0.3722 - acc: 0.8386 - val_loss: 0.4958 - val_acc: 0.7564\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.3517 - acc: 0.8506 - val_loss: 0.4987 - val_acc: 0.7578\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.3326 - acc: 0.8606 - val_loss: 0.5058 - val_acc: 0.7554\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.3145 - acc: 0.8687 - val_loss: 0.5129 - val_acc: 0.7576\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.2974 - acc: 0.8784 - val_loss: 0.5205 - val_acc: 0.7540\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整合：从原始文本到词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_dir = r'D:\\python\\deep learning\\data\\aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\python\\\\deep learning\\\\data\\\\aclImdb\\\\train'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels =[]\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname),'rb')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](./images/RNN.png)   \n",
    "   RNN:遍历所有的序列元素，并保存一个状态（status）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RNN实现的伪代码\n",
    "state_t = 0\n",
    "for input_t in input_sequence:\n",
    "    #output_t = f(input_t, state_t)\n",
    "    output_t = activation( dot(W, input_t) + dot(U, state_t) + b )\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建一个带RNN的网络\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 328,320\n",
      "Trainable params: 328,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用上述模型应用于IMDB电影评论分类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "25000 train sequence\n",
      "25000 test sequence\n",
      "Pad sequences (samples x time)\n",
      "input_train shape (25000, 500)\n",
      "input_test shape (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "max_features = 10000           # 作为特征的单词个数\n",
    "maxlen = 500                   # 每个文本只读前500个单词\n",
    "batch_size = 32\n",
    "\n",
    "print('loading data...')\n",
    "(input_train, y_train),(input_test, y_test) = imdb.load_data(num_words = max_features)\n",
    "\n",
    "print(len(input_train), 'train sequence')\n",
    "print(len(input_test), 'test sequence')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "\n",
    "print('input_train shape', input_train.shape)\n",
    "print('input_test shape', input_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用一个Embedding 层和一个SimpleRNN层来训练一个简单的循环网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.6429 - acc: 0.6137 - val_loss: 0.6302 - val_acc: 0.6434\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.4235 - acc: 0.8136 - val_loss: 0.5158 - val_acc: 0.7466\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.3152 - acc: 0.8750 - val_loss: 0.4348 - val_acc: 0.7994\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.2382 - acc: 0.9083 - val_loss: 0.4079 - val_acc: 0.8318\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1707 - acc: 0.9361 - val_loss: 0.3773 - val_acc: 0.8656\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.1119 - acc: 0.9613 - val_loss: 0.3961 - val_acc: 0.8572\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0753 - acc: 0.9763 - val_loss: 0.4668 - val_acc: 0.8378\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0432 - acc: 0.9876 - val_loss: 0.5835 - val_acc: 0.8142\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0242 - acc: 0.9927 - val_loss: 0.5762 - val_acc: 0.8392\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.0188 - acc: 0.9946 - val_loss: 0.6459 - val_acc: 0.8230\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleRNN最大问题，在时刻t,理论上应该能记住许多时间之前的信息，但实际上不会，原因在于**梯度消失问题**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM是RNN的一种变种：保存信息以便后面使用，从而防止较早期的信号在处理过程中逐渐消失。   \n",
    "![SimpleRNN](./images/simplernn.png)   \n",
    "![LSTM](./images/LSTM.png)   \n",
    "![LSTM解剖](./images/LSTM解剖.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM伪代码：   \n",
    "output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + b)\n",
    "\n",
    "i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)   \n",
    "f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)   \n",
    "k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)   \n",
    "   \n",
    "c_t+1 = i_t \\* k_t + c_t \\* f_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
